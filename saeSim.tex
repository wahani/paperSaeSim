\documentclass[article]{ajs}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% declarations for jss.cls %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% additional packages
\usepackage{wrapfig}
\usepackage{graphicx}
\usepackage{subcaption}

%% definitions of entities in formulae
\include{mathOperators}

%% almost as usual
\author{Sebastian Warnholz\\ Freie Universit\"at Berlin \And 
        Timo Schmid \\ Freie Universit\"at Berlin}
\title{Simulation Tools for Small Area Estimation: Introducing the \proglang{R}-package \proglang{saeSim}}

%% for pretty printing and a nice hypersummary also set:
\Plainauthor{Sebastian Warnholz, Timo Schmid} %% comma-separated
\Plaintitle{Simulation Tools for Small Area Estimation: Introducing the R-package saeSim} %% without formatting
\Shorttitle{Simulation Tools for Small Area Estimation} %% a short title (if necessary)

%% an abstract and keywords
\Abstract{
  The abstract of the article in English
  
}
\Keywords{package, small area estimation, reproducible research, simulation, \proglang{R}}
\Plainkeywords{package, small area estimation, reproducible research, simulation, R} %% without formatting
%% at least one keyword must be supplied

%% publication information
%% NOTE: Typically, this can be left commented and will be filled out by the technical editor
%% \Volume{50}
%% \Issue{9}
%% \Month{June}
%% \Year{2012}
%% \Submitdate{2012-06-04}
%% \Acceptdate{2012-06-04}
%% \setcounter{page}{1}
\Pages{1--xx}

%% The address of (at least) one author should be given
%% in the following format:
\Address{
  Sebastian Warnholz\\
  Department of Economics\\
  Freie Universit\"at Berlin\\
  D-14195 Berlin, Germany\\
  E-mail: \email{Sebastian.Warnholz@fu-berlin.de}\\
  URL: \url{http://www.wiwiss.fu-berlin.de/fachbereich/vwl/Schmid/Team/Warnholz.html}
}
%% It is also possible to add a telephone and fax number
%% before the e-mail in the following format:
%% Telephone: +43/512/507-7103
%% Fax: +43/512/507-2851

%% for those who use Sweave please include the following line (with % symbols):
%% need no \usepackage{Sweave.sty}

%% end of declarations %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{document}
%
%% include your article here, just as usual
%
% R CMD Sweave %.Rnw
% Loading (and installing) necessary packages.
%
\section{Introduction}
The objective of small area estimation is to produce reliable statistics (means, quantiles, proportions, etc.) for domains where little or no sampled units are available. Groups may be areas or other entities, for example defined by socio-economic characteristics. New statistical methods are applied in model-based and design-based simulation studies. Considering the demands for reproducible research we propose a framework for simulation studies inside the field of small area estimation.

Reproducible Research has become a widely discussed topic in general and also in the field of statistics. Thanks to the many mostly open-source tools like the \proglang{R}-language \citep{r14} and \LaTeX, and also packages like knitr \citep{yihui13} and Sweave \citep{leisch02} and more recently rmarkdown \citep{allaire14}, the integration of text and source code for statistical analysis is possible. Not only are tools available to make research reproducible, also the demand of making the analysis of articles reproducible is rising which is supported by the rising number of Open Access Journals. Publishing source-code and data alongside an article draws special attention to authoring an analysis. However, the requirements for source code are different from the written words in the article itself. 

\begin{quote}
\textit{Instead of imagining that our main task is to instruct a computer what to do, let us concentrate rather on explaining to human beings what we want a computer to do.} \cite[p.99]{knuth92} 
\end{quote}

In this article we want to introduce a new package for the \proglang{R}-language to support the process of making simulation studies reproducible as well as supporting a \textit{human-readable} interface. With this package we have three objectives in specific: First, making tools for data generation available and reusable. Second, unify the process behind simulation studies inside the field of Small Area Estimation. And third, making source-code of simulation studies available, such that it supports the conducted research in a transparent manner.

In section \ref{sec:SAE} we will give a short introduction to small area estimation and the role of simulation studies within that field. Section \ref{sec:framework} will then introduce a framework for simulation studies and how it is supported by \proglang{saeSim}. To illustrate some of the package's features we present a case study in section \ref{sec:caseStudy}.

\section{Small area estimation}
\label{sec:SAE}
The objective of small area estimation is to produce reliable statistics (means, quantiles, proportions, etc.) for domains where little or no sampled units are available. Groups may be areas or other entities, for example defined by socio-economic characteristics. The demand for such estimators is rising as they are used for fund allocation, educational and health programs \citep{pfeffermann13}. As direct estimation of such statistics are considered to be unreliable, methods in small area estimation try to improve the domain predictions by borrowing strength from neighboured or \textit{similar} domains. This can be achieved by using additional information from census data to assist the prediction for non-sampled domains or domains with little information. For the purpose of this article we will introduce two basic models frequently used in small area estimation, the unit-level model introduced by \cite{battese88} and the area-level model introduced by \cite{fay79}.

The unit level model \citep{battese88} can be expressed as:
\begin{eqnarray}
	 y_{\indexDomain\indexUnit} =& \xUnit^\top\beta + \randomEffectIndexed + \samplingErrorUnitIndexed \nonumber \\
	\randomEffectIndexed \stackrel{iid}{\sim}& N(0, \randomEffectVariance)  \nonumber \\
	\samplingErrorUnitIndexed \stackrel{iid}{\sim}& N(0, \samplingVariance) \nonumber
\end{eqnarray}

where $\indexDomain = 1, \dots, \nDomains$ and $\indexUnit = 1, \dots, \nUnitIndexed$. $y_{\indexDomain\indexUnit}$ is the the dependent variable for domain $\indexDomain$ and unit $\indexUnit$, and $\xUnit$ the corresponding auxiliary information for that unit. Furthermore $\randomEffect$ and $\samplingError$ are independent. This model can be seen as a linear mixed model from which the best linear unbiased predictor (BLUP) can be derived and is used for the domain prediction \citep{rao03}.

Due to reasons of confidentiality unit-level information is not always available. Instead only aggregates, or rather the direct estimators may be supplied. However, these direct estimations are known to be unreliable, hence in such situations area-level models can be valuable. The area-level model introduced by \cite{fay79} is build on a sampling model:
%
\[\directStat_{\indexDomain} = \trueStat_{\indexDomain} + \samplingError_{\indexDomain},\]
%
where $\directStat_{\indexDomain}$ is a direct estimator of a statistic of interest $\trueStat_{\indexDomain}$ for an area $\indexDomain$ with $\indexDomain = 1, \dots, \nDomains$ and $\nDomains$ being the total number of areas. The sampling error $\samplingError_{\indexDomain}$ is assumed to be independent and normally distributed with known variances $\samplingVarianceIndexed$, i.e. $\samplingError_{\indexDomain}|\trueStat_{\indexDomain} \sim \mathit{N}(0, \samplingVarianceIndexed)$. The model is modified with the linking model by assuming a linear relationship between the true area statistic $\trueStat_{\indexDomain}$ and some auxiliary variables $\xArea$:
%
\[\trueStat_{\indexDomain} = \xArea^\top \beta + \randomEffectIndexed,\] 
%
with $\indexDomain=1,\dots, \nDomains$. Note that $\xArea$ is a vector containing area-level (aggregated) information for $\nRegressor$ variables and $\beta$ is a vector ($1\times \nRegressor$) of regression coefficients describing the (linear) relationship. The model errors $\randomEffectIndexed$ are assumed to be independent and normally distributed, i.e. $\randomEffectIndexed \sim \mathit{N}(0, \randomEffectVariance)$. Furthermore $\samplingErrorIndexed$ and $\randomEffectIndexed$ are assumed to be independent. Combining the sampling and linking model leads to:
\begin{equation}
\label{eq:FH}
\directStatIndexed = \xArea^\top \beta + \randomEffectIndexed + \samplingErrorIndexed.
\end{equation} 
%
Model \ref{eq:FH} is effectively a random-intercept model where the distribution of the error term $\samplingErrorIndexed$ is heterogeneous and known. 

\section{A simulation framework}
\label{sec:framework}
%
%% flow-diagram
\begin{wrapfigure}{R}{0.5\textwidth}
\begin{center}
\includegraphics[width=0.5\textwidth]{flowdiagram}
\end{center}
\caption{\label{fig:flowdiagram}Process of simulation.} 
\end{wrapfigure}
%
In our opinion simulation studies can best be summarised when understood as a process of data manipulation. Thus the main focus of \proglang{saeSim} is to define the steps in that process which can then be \textit{cleanly} defined and repeated. Before we go into any detail of the functionality of the package we will discuss the process behind simulation studies and later how \proglang{saeSim} maps this process into \proglang{R}.

Simulation studies in small area estimation address three different levels, the population, the sample and data on aggregated level, as illustrated in figure \ref{fig:flowdiagram}. The \textbf{population-level} defines the data on which a study is conducted and may be a true population, synthetic population data or randomly generated variates from a model. We see three different point of views to define a population. First \textit{design-based}, which means that a simulation study is based on true or synthetic data of \textit{one} population. Second a \textit{semi-model-based} point of view, where only one population is drawn from a model and is fixed in the whole simulation study. And third, \textit{model-based} studies which have changing random populations drawn from a model.

The scope of this article is not not to promote any of those viewpoints, but simply to identify the similarity in them. The \textit{base} (first component in figure \ref{fig:flowdiagram}) of any simulation study is a data table, the question is, if this data is \textit{fixed} or \textit{random} over the course of the simulation. Or from a more technical point of view, is the data generation (the second step in figure \ref{fig:flowdiagram}) repeated in each simulation run or omitted?

Depending on the choice of a fixed or random population it is necessary to recompute the population domain-statistics like domain means and variances, or other statistics of interest (third component in figure \ref{fig:flowdiagram}).

The \textbf{sample-level} is when domain predictions are conducted for unit-level models. Independently of how the population is treated, fixed or random, this phase consists of two steps, first drawing a sample, and second conducting computations on the samples (fourth and fifth component in figure \ref{fig:flowdiagram}). Given a sample, design or model based small area methods are applied. Of interest are estimated parameters, which can be estimated model parameters or domain predictions as well as measures of uncertainty for the estimates.

As the sample-level is when unit-level models are applied, the \textbf{aggregate-level} is when area-level models are applied (the seventh and last component in figure \ref{fig:flowdiagram}). Area-level models in small area estimation typically only use information available for domains (in contrast to units). Thus, the question for simulation studies for area-level methods is, if the data is generated on unit-level and used after the aggregation (sixth component in figure \ref{fig:flowdiagram}) or if the data is generated directly on area-level, i.e. drawn from an area-level model. Depending on whether unit-level data and sampling are part of the simulation the aggregate-level follows the generation of the population or is based on the aggregated sample. Again, we do not promote a specific viewpoint but simply allow steps in the process of simulation to be omitted.

Depending on the topic of research steps in this simulation framework can be more relevant than others or completely irrelevant. We see these steps more as a complete list of phases one can encounter, thus single components can be omitted if not relevant in specific applications. For example \textit{data generation} is not relevant if you have population data, or the \textit{sample-level} is not used, when the sample is directly drawn from the model.

From this considerations, \proglang{saeSim} maps the different steps into \proglang{R}. Two layers with separate responsibilities need to be discussed. The first is \textit{how} different simulation components can be combined, and the second \textit{when} or in which order they are applied. Regarding the first, in \proglang{saeSim} we put a special emphasis on the interface of each component, which is to use functions which take a \proglang{data.frame} as argument and have a \proglang{data.frame} as return value. This is a widely used approach for data manipulation and promoted recently in the package \proglang{dplyr} \citep{wickham14}. This definition of interfaces, the return value of one component is the input of the next, is also used in \proglang{saeSim}.

Understanding a simulation as a process of manipulating one data object, see the second column in figure \ref{fig:flowdiagram} how the different steps in a simulation can be accessed. It is important to note that the functions in figure \ref{fig:flowdiagram} control the process, the second layer, i.e. \textit{when} components are applied. Each of these functions take a simulation setup object to be modified and a function with the discussed interface as arguments. Hence a simulation setup is a collection of functions to be applied in a certain sequence. Also the second-layer functions have a defined interface: a \proglang{sim\_setup} as input to be modified and a \proglang{sim\_setup} as output. Thus, components can be chained together using the \textit{pipe operator} (\proglang{\%>\%}) from the package \proglang{magrittr} \citep{bache14}.

With \proglang{saeSim} we want to contribute tools for simulation studies in the field of small area estimation. We see the need for sharing tools for data generation and simulation amongst the scientific community and thus defined an interface for these tools as well as a platform to make them accessible. By defining the steps behind a simulation we hope to promote a reasonable way to communicate them alongside publications and during research. In the next section we will present a case study and focus more on the concrete functionality provided by the package.

\section{Case study}
Braking the responsibility into what is applied and when it is applied addresses several aspects of reproducibility. First, by defining the interface between all components, it is easy to combine them in any combination and thus easy to share and reuse. Second, by controlling when components are applied we avoid the necessity of control structures in syntax and emphasise on the definition of components. The following example shows these aspects of the package using a predefined simulation setup:

\begin{Schunk}
\begin{Sinput}
> setup1 <- sim_base_lm() %>% sim_sample(sample_number(5))
> setup2 <- sim_base_lm() %>% sim_sample(sample_fraction(0.05))
\end{Sinput}
\end{Schunk}

Without knowing anything about the setup defined in \proglang{sim\_base\_lm} we can see that \proglang{setup1} and \proglang{setup2} only differ in the applied sampling scheme. \proglang{sim\_sample} is responsible to control when a function is applied (after the population-level) and \proglang{sample\_number(5)} and \proglang{sample\_fraction(0.05)} define the explicit way of drawing samples. The pipe operator \proglang{\%>\%} is used to add new components to the setup. As said before the composition of a simulation in that manner will focus on the definition of components and hide control structures. The next example will repeat the simulation stored in \proglang{setup1} two times. The results are returned in a list of \proglang{data.frame}s which have five rows as we sample 5 observations:

\begin{Schunk}
\begin{Sinput}
> setup1 %>% sim(R = 2) %>% sapply(nrow)
\end{Sinput}
\begin{Soutput}
[1] 5 5
\end{Soutput}
\end{Schunk}

As a side remark: The pipe operator, \proglang{\%>\%}, is designed to make otherwise nested expressions more readable as a line can be read from left to right, instead of inside out \citep{bache14}. As a minimal example see the following lines which are equivalent:

\begin{Schunk}
\begin{Sinput}
> sum(1:10)
\end{Sinput}
\begin{Soutput}
[1] 55
\end{Soutput}
\begin{Sinput}
> 1:10 %>% sum
\end{Sinput}
\begin{Soutput}
[1] 55
\end{Soutput}
\end{Schunk}


\label{sec:caseStudy}
\subsection{Model-based simulation}


The first task for the model-based simulation is to generate the data under the model:

\[ y_i = 100 + 2 \cdot x_i + v_i + e_i\]

where $x_i \stackrel{iid}{\sim} N(0, 4^2)$, $v_i \stackrel{iid}{\sim} N(0, 1)$ and $e_i \stackrel{indep}{\sim} N(0, \sigma_i^2)$ with $\sigma_i^2 = 0.1, 0.2, \dots, 4$ and $i = 1, \dots, 40$ as index for the domains. Also are $x_i$, $v_i$ and $e_i$ independent from each other. The starting point is always a data table as \textit{base-component}, in this case just with an id variable named \proglang{idD} and constructed with the function \proglang{base\_id}. Any random number generator in \proglang{R} can be used, however we have a strictly \textit{normal} model, for which some predefined functions can be used.

\begin{Schunk}
\begin{Sinput}
> library(saeSim)
\end{Sinput}
\end{Schunk}
\begin{Schunk}
\begin{Sinput}
> setup <- base_id(nDomains = 40, nUnits = 1) %>% 
+   sim_gen_x(mean = 0, sd = 4) %>%
+   sim_gen_v(mean = 0, sd = 1)
> setup
\end{Sinput}
\begin{Soutput}
  idD          x          v
1   1 -2.5058152 -0.1645236
2   2  0.7345733 -0.2533617
3   3 -3.3425144  0.6969634
4   4  6.3811232  0.5566632
5   5  1.3180311 -0.6887557
6   6 -3.2818735 -0.7074952
\end{Soutput}
\end{Schunk}

Note that if you print a simulation setup to the console, as in the above example, one simulation run is performed and only the first rows of the resulting data table are printed. This enables interactivity with the object itself, however it hides that the setup object is a collection of functions to be called. The error component $e_i$ in the model has different variances which is not covered by a predefined function. Thus, as a \textit{generator} function we will define a function which takes a \proglang{data.frame} as input and returns it after adding a variable named \proglang{vardir} with the variances and the variable \proglang{e} with the generated random numbers:

\begin{Schunk}
\begin{Sinput}
> gen_e <- function(dat) {
+   dat$vardir <- seq(0.1, 4, length.out = nrow(dat))
+   dat$e <- rnorm(nrow(dat), sd = sqrt(dat$vardir))
+   dat
+ }
> setup <- setup %>% sim_gen(gen_e)
> setup
\end{Sinput}
\begin{Soutput}
  idD          x          v vardir          e
1   1 -2.2746749 -0.5059575    0.1  0.1344285
2   2 -0.5407145  1.3430388    0.2 -0.1067262
3   3  4.7123480 -0.2145794    0.3  0.5797550
4   4 -6.0942672 -0.1795565    0.4  0.5606229
5   5  2.3757848 -0.1001907    0.5 -0.4378710
6   6  1.3318015  0.7126663    0.6  1.7088396
\end{Soutput}
\end{Schunk}

The last step in data generation is to construct the response variable which will be named \proglang{y} and added to the data. Also we will add the \textit{true} area statistic under the model to the data:

\begin{Schunk}
\begin{Sinput}
> setup <- setup %>% 
+   sim_resp_eq(y = 100 + 2 * x + v + e) %>%
+   sim_comp_pop(comp_var(trueStat = y - e))
\end{Sinput}
\end{Schunk}

Not all necessary functions can be provided by the package. As said before, one of the main objectives was to define the interface between components. To add the area-level predictions from a Fay-Herriot model we need to define a function for that. The function takes a \proglang{data.frame} as input and returns the modified version. Here we use the function \proglang{eblupFH} from the package \proglang{sae} \citep{molina13} to estimate the EBLUP under the FH model. Finally we add \proglang{comp\_FH} to the process:

\begin{Schunk}
\begin{Sinput}
> library(sae)
\end{Sinput}
\end{Schunk}
\begin{Schunk}
\begin{Sinput}
> comp_FH <- function(dat) {
+   modelFH <- eblupFH(y ~ x, vardir, data = dat)
+   dat$FH <- modelFH$eblup
+   dat
+ }
> setup <- setup %>% sim_comp_agg(comp_FH)
> setup
\end{Sinput}
\begin{Soutput}
  idD         x          v vardir          e         y  trueStat        FH
1   1  1.637607  0.7073107    0.1  0.1258998 104.10843 103.98253 104.06121
2   2  6.755493  1.0341077    0.2 -0.1822523 114.36284 114.54509 114.23876
3   3  6.346354  0.2234804    0.3  0.7253263 113.64151 112.91619 113.45213
4   4 -1.323631 -0.8787076    0.4 -0.4434978  96.03053  96.47403  96.45416
5   5 -9.140942  1.1629646    0.5 -0.4105563  82.47052  82.88108  82.46045
6   6  9.990646 -2.0001649    0.6 -0.7754272 117.20570 117.98113 118.08379
\end{Soutput}
\end{Schunk}

The object \proglang{setup} stores all necessary information to run one iteration of the simulation. To initialize more repetition, \proglang{sim} is called, which will return $R = 100$ (as the number of repetitions) \proglang{data.frame}s containing the simulated data from each iteration as a \proglang{list}. We use \proglang{rbind\_all} from the package \proglang{dplyr} to append the resulting \proglang{list}:

\begin{Schunk}
\begin{Sinput}
> library(dplyr)
\end{Sinput}
\end{Schunk}
\begin{Schunk}
\begin{Sinput}
> simResults <- sim(setup, R = 100) %>%rbind_all
> simResults %>% select(idD, idR, trueStat, y, FH)
\end{Sinput}
\begin{Soutput}
Source: local data frame [4,000 x 5]

   idD idR  trueStat         y        FH
1    1   1 107.87088 108.21065 108.30528
2    2   1 113.29033 114.13809 113.80733
3    3   1 105.88208 105.55180 105.63108
4    4   1  84.61171  84.36450  84.63272
5    5   1 103.68692 103.39260 103.42371
6    6   1 104.26130 103.97032 103.79131
7    7   1  97.85114  97.54439  97.43298
8    8   1 101.93187 101.66741 101.49500
9    9   1  92.51517  93.88300  93.67397
10  10   1 104.07055 103.37302 104.01407
.. ... ...       ...       ...       ...
\end{Soutput}
\end{Schunk}

An additional variable \proglang{idR} is automatically added as an ID-variable for the iteration. At this time we do not provide further tools to process the resulting data. As it is a \proglang{data.frame} many packages for processing that data are available in \proglang{R}, in this case \proglang{dplyr} \citep{wickham14} and \proglang{reshape2} \citep{wickham07} have been used. The Rmd-file for this article can be checked for further detail, thus we do not see need to provide further tools from here on.




\begin{figure}[h!]
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=.9\linewidth]{saeSim-modelSimBIAS.pdf}
\caption{}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=.9\linewidth]{saeSim-modelSimRMSE.pdf}
\caption{}
\end{subfigure}
\caption{RBIAS and RMSE for the conducted model-based simulation}
\end{figure}

\subsection{Design-based simulation}

In the design-based simulation we want to illustrate the use of \proglang{saeSim} when starting from data of a population. For this purpose we use a synthetic population generated from Austrian EU-SILC (European Union Statistics on Income and Living Conditions) data. We use this data to illustrate the functionality of the package, the data consists of 25 thousand households and does not represent the true population size of Austria. It is published alongside the \proglang{R}-package \proglang{simFrame} \citep{alfons10} where it is used as example data.

\begin{Schunk}
\begin{Sinput}
> data(eusilcP, package = "simFrame")
> simDat <- eusilcP %>% 
+   mutate(agesq = age^2, eqIncome = as.numeric(eqIncome)) %>%
+   filter(main) %>%
+   select(region, eqIncome, age, agesq, gender) %>%
+   base_add_id(c("region"))
> attr(simDat, "popMeans") <- 
+   simDat %>% 
+   group_by(idD, region) %>%
+   summarise(age = mean(age),
+             agesq = mean(agesq),
+             genderFemale = mean(as.integer(gender) - 1),
+             trueStat = mean(eqIncome))
> attr(simDat, "popN") <- 
+   simDat %>%
+   group_by(idD) %>%
+   summarise(N = n())
> attr(simDat, "popN")
\end{Sinput}
\begin{Soutput}
Source: local data frame [9 x 2]

  idD    N
1   1  799
2   2 4619
3   3 5857
4   4 1723
5   5 3386
6   6 4071
7   7 1671
8   8 1889
9   9  985
\end{Soutput}
\begin{Sinput}
> attr(simDat, "popMeans")
\end{Sinput}
\begin{Soutput}
Source: local data frame [9 x 6]
Groups: idD

  idD        region      age    agesq genderFemale trueStat
1   1    Burgenland 54.50063 3269.677    0.3366708 22005.42
2   2 Lower Austria 51.95259 3009.934    0.3777874 19813.37
3   3        Vienna 46.98310 2486.448    0.4662797 20395.84
4   4     Carinthia 51.81428 2995.735    0.3540337 19486.18
5   5        Styria 50.64087 2886.845    0.3573538 19335.39
6   6 Upper Austria 50.18644 2795.804    0.3443871 20517.29
7   7      Salzburg 51.44943 2965.268    0.4189108 19890.33
8   8         Tyrol 51.76707 2995.451    0.3975648 19350.89
9   9    Vorarlberg 49.06904 2697.382    0.3583756 22156.12
\end{Soutput}
\begin{Sinput}
> 
\end{Sinput}
\end{Schunk}

\begin{Schunk}
\begin{Sinput}
> comp_BHF <- function(dat) {
+   popMeans <- attr(dat, "popMeans") %>% select(-trueStat, -region)
+   modelBHF <- 
+     eblupBHF(eqIncome ~ age + agesq + gender, idD, meanxpop = popMeans,
+              popnsize = attr(dat, "popN"), data = dat)
+   attr(dat, "BHF") <- modelBHF$eblup
+   dat
+ }
> comp_direct <- function(dat) {
+   attr(dat, "sampleMean") <- 
+     dat %>% group_by(idD) %>% summarise(y = mean(eqIncome))
+   dat  
+ }
> agg_results <- function(dat) {
+   cbind(attr(dat, "popMeans") %>% select(idD, region, trueStat), 
+         BHF = attr(dat, "BHF")$eblup,
+         y = attr(dat, "sampleMean")$y)
+ }
\end{Sinput}
\end{Schunk}

\begin{Schunk}
\begin{Sinput}
> simResults <- simDat %>% 
+   sim_sample(sample_fraction(0.1, groupVars = "idD")) %>%
+   sim_comp_sample(comp_BHF) %>%
+   sim_comp_sample(comp_direct) %>%
+   sim_agg(agg_results) %>%
+   sim(R = 50) %>%
+   rbind_all
> ggDat <- simResults %>% 
+   melt(id.vars = c("idD", "region", "trueStat"), 
+        measure.vars = c("BHF", "y"), 
+        variable.name = "method",
+        value.name = "prediction")
\end{Sinput}
\end{Schunk}

\begin{Schunk}
\begin{Sinput}
> ggplot(ggDat, aes(x = region, y = prediction - trueStat, fill = method)) + 
+   geom_boxplot() + theme(legend.position = "bottom")
\end{Sinput}
\end{Schunk}

\includegraphics[width = \textwidth]{saeSim-designSimBIAS.pdf}

\begin{Schunk}
\begin{Sinput}
> ggplot(ggDat, aes(x = region, y = (prediction - trueStat)^2, fill = method)) + 
+   geom_boxplot() + scale_y_log10() + theme(legend.position = "bottom")
\end{Sinput}
\end{Schunk}

\includegraphics[width = \textwidth]{saeSim-designSimMSE.pdf}
  

  
\section{Outlook}
Use this package to share and publish simulation studies alongside papers. Contribute to the package to make your ideas available. Contribute to the package and make your whole simulation study available. 
\begin{itemize}
	\item Link to simFrame
	\item Package Features
	\begin{itemize}
		\item outliers
		\item sampling
		\item non-linear models
	\end{itemize}
	\item How to contribute?
	\item parallel computations
\end{itemize}


%\bibliographystyle{plainat}
\bibliography{saeSim}



\end{document}
