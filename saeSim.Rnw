\documentclass[article]{ajs}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% declarations for jss.cls %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% additional packages
\usepackage{wrapfig}
\usepackage{graphicx}
\usepackage{subcaption}

%% definitions of entities in formulae
\include{mathOperators}

%% almost as usual
\author{Sebastian Warnholz\\ Freie Universit\"at Berlin \And 
        Timo Schmid \\ Freie Universit\"at Berlin}
\title{Simulation Tools for Small Area Estimation: Introducing the \proglang{R}-package \proglang{saeSim}}

%% for pretty printing and a nice hypersummary also set:
\Plainauthor{Sebastian Warnholz, Timo Schmid} %% comma-separated
\Plaintitle{Simulation Tools for Small Area Estimation: Introducing the R-package saeSim} %% without formatting
\Shorttitle{Simulation Tools for Small Area Estimation} %% a short title (if necessary)

%% an abstract and keywords
\Abstract{
  In this article the \proglang{R}-package \proglang{saeSim} is introduced. The package provides a simulation framework for studies in the field of small area estimation. As the demand for reliable regional estimates from sample surveys has been substantially grown, small area estimation provides statistical methods to produce reliable predictions when the sample sizes in regions are small. Model- and design-based simulations are used to introduce new methods to the field. In this artical a reproducible research approach towards the publication of simulation studies alongside articles and during research is promoted. The package aims to assist the researcher during the research process and as a vessel to publish new tools and studies.  
}
\Keywords{package, \proglang{R}, reproducible research, simulation, small area estimation}
\Plainkeywords{package, R, reproducible research, simulation, small area estimation} %% without formatting
%% at least one keyword must be supplied

%% publication information
%% NOTE: Typically, this can be left commented and will be filled out by the technical editor
%% \Volume{50}
%% \Issue{9}
%% \Month{June}
%% \Year{2012}
%% \Submitdate{2012-06-04}
%% \Acceptdate{2012-06-04}
%% \setcounter{page}{1}
\Pages{1--xx}

%% The address of (at least) one author should be given
%% in the following format:
\Address{
  Timo Schmid\\
  Department of Economics\\
  Freie Universit\"at Berlin\\
  D-14195 Berlin, Germany\\
  E-mail: \email{Timo.Schmid@fu-berlin.de}\\
  URL: \url{http://www.wiwiss.fu-berlin.de/fachbereich/vwl/Schmid}\\
  
  Sebastian Warnholz\\
  Department of Economics\\
  Freie Universit\"at Berlin\\
  D-14195 Berlin, Germany\\
  E-mail: \email{Sebastian.Warnholz@fu-berlin.de}\\
  URL: \url{http://www.wiwiss.fu-berlin.de/fachbereich/vwl/Schmid/Team/Warnholz.html}
}


%% It is also possible to add a telephone and fax number
%% before the e-mail in the following format:
%% Telephone: +43/512/507-7103
%% Fax: +43/512/507-2851

%% for those who use Sweave please include the following line (with % symbols):
%% need no \usepackage{Sweave.sty}

%% end of declarations %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{document}
%
%% include your article here, just as usual
%
% R CMD Sweave %.Rnw
% Loading (and installing) necessary packages.
<<echo=FALSE>>=
loadAndInstallPackage <- function(packageName) {
  if(!eval(substitute(suppressPackageStartupMessages(require(packageName))))) {
    install.packages(packageName)
    eval(substitute(suppressPackageStartupMessages(library(packageName))))
  } 
}
loadAndInstallPackage("saeSim")
# Keep order in which packages are loaded, there are naming conflicts between dependencies (MASS and dplyr):
loadAndInstallPackage("sae")
loadAndInstallPackage("dplyr")
loadAndInstallPackage("reshape2")
loadAndInstallPackage("ggplot2")
@
%
\section{Introduction}
The demand for reliable small area statistics from sample surveys has been substantially grown over the last decades due to their use in public and private sectors. In this paper we present a framework for simulation studies inside the field of small area estimation. This tool might be useful for the prospective researcher or data analyst to provide reproducible research.

Reproducible research has become a widely discussed topic. In the field of statistics many mostly open-source tools like the \proglang{R}-language \citep{r14} and \LaTeX, dynamic reporting packages like \proglang{knitr} \citep{yihui13}, \proglang{sweave} \citep{leisch02} and more recently \proglang{rmarkdown} \citep{allaire14}, make the integration of text and source code for statistical analysis possible. Publishing source code and data alongside research results draws special attention to authoring the analysis. However, the requirements for source code are different from the written words in the article itself. 

\begin{quote}
\textit{Instead of imagining that our main task is to instruct a computer what to do, let us concentrate rather on explaining to human beings what we want a computer to do.} \cite[p.99]{knuth92} 
\end{quote} 

Next to the combination of text and source code, reproducible research aims that the full output of the academic research which is the paper combined with the full computational environment like data and source code, is available. However, real data is often very sensitive and governed by strict confidentiality rules. Synthetic data generation mechanisms \citep{Kol11} can be used to provide safe data which is publicly available to enable the community to reproduce the analysis and results. \cite{Bur14} interpreted this as an open research philosophy. Such synthetic data sets can be used to test newly proposed statistical methods in a close-to-reality framework. 
%This may provide valuable insights about the quality of the introduced methods in a controlled environment under different scenarios. 
In general, statistical simulation studies can be divided into two concepts:
\begin{itemize}
\item Design-based: The simulation study is based on true or synthetic data of a fixed population. Then, samples are selected repeatedly from the underlying finite population and different estimation methods are applied in each replication. The obtained estimates are compared to the true values of the population, for instance, in terms of relative bias (RB) or relative root mean squared error (RRMSE).
\item Model-based: The simulation study uses data drawn from certain distributions. In each iteration, the population is generated from a model and a sample is selected according to a specific sampling scheme. The sample is used to estimate the quantity of interest and quality measures (like RB and RRMSE) are derived.  
\end{itemize}
Further discussion regarding model- and design-based simulations is available in \cite{Mue03}, \cite{Sal10} or \cite{Alf10}.

%However, simulation studies are often presented very briefly in academic articles without providing enough details about the underlying structure of the study. 
\cite{Alf10} provide the \proglang{R}-package \proglang{simFrame} which helps to conduct simulation studies in a reproducible environment. It includes a wide range of different features (like data generation, sampling schemes, outlier contamination mechanisms and missing values) to conduct simulation studies. \proglang{simFrame} was originally developed for simulations in the context of survey statistics but is now designed to be as general as possible \citep[cf.][]{Alf10}.

Survey statistics are used, for example, in order to deliver specific indicators as a basis for economic and political decision processes. Especially regional or group-specific comparisons are of interest \citep[cf.][]{Sch13}. Surveys which shall provide the sufficient data for these regional indicators, however, are generally designed for larger areas (NUTS 1-2 level). Hence, sample information on more detailed levels, like NUTS3, is hardly available so that classical estimation methods (direct estimators) may lead to high variances of the estimates \citep[cf.][]{Gho94}. In this case, small area estimation methods may reveal highly improved results for the target estimates. Small area estimation has become more and more attractive over the last decade: 

\begin{quote}
\textit{In 2002, small area estimation (SAE) was flourishing both in research and applications, but my own feeling then was that the topic has been more or less exhausted in terms of research and that it will just turn into a routine application in sample survey practice. As the past 9 years show, I was completely wrong; not only is the research in this area accelerating, but it now involves some of the best known statisticians...} \cite{pfeffermann13} 
\end{quote} 

However, simulation studies in the context of small area estimation is often presented very briefly . Thus, we see the need to have a suited framework to guarantee the reproducibility of analysis. To the best of our knowledge, there is not any \proglang{R}-package or framework adjusted for the special case of small area estimation which provides a simulation environment.

The aim of this article is to introduce a new \proglang{R}-package, \proglang{saeSim}, which supports the process of making simulation studies in the field of small area estimation reproducible. To be more precise, the package has three main objectives: First, provide tools for data generation. Second, unify the process of simulation studies. Third, make the source-code of simulation studies available, such that it supports the conducted research in a transparent manner.

The paper is organised as follows. In Section \ref{sec:SAE} we give a short introduction to small area estimation focusing mainly on unit-level \citep{battese88} and area-level models \citep{fay79}. Section \ref{sec:framework} introduces a framework for simulation studies and how it is supported by the  \proglang{R}-package \proglang{saeSim}. To illustrate some of the features of the package we present a case study in Section \ref{sec:caseStudy}. We conclude the paper in Section \ref{sec:outlook} by summarising the main findings and by providing some avenues for further research.


\section{Small area estimation}
\label{sec:SAE}

The objective of small area estimation is to produce reliable statistics (means, quantiles, proportions, etc.) for domains where little or no sampled units are available. Groups may be areas or other entities, for example defined by socio-economic characteristics. The demand for such estimators is rising as they are used for fund allocation, educational and health programs \citep{pfeffermann13}. As direct estimation of such statistics are considered to be unreliable, methods in small area estimation try to improve the domain predictions by borrowing strength from neighboured or \textit{similar} domains. This can be achieved by using additional information from census data or registers to assist the prediction for non-sampled domains or domains with little information. 

For the purpose of this article we will introduce two basic models frequently used in small area estimation, the unit-level model introduced by \cite{battese88} and the area-level model introduced by \cite{fay79}. The unit level model \citep{battese88} can be expressed as:
\begin{eqnarray}
	 y_{\indexDomain\indexUnit} =& \xUnit^\top\beta + \randomEffectIndexed + \samplingErrorUnitIndexed \nonumber \\
	\randomEffectIndexed \stackrel{iid}{\sim}& N(0, \randomEffectVariance)  \nonumber \\
	\samplingErrorUnitIndexed \stackrel{iid}{\sim}& N(0, \samplingVariance) \nonumber
\end{eqnarray}

where $\indexDomain = 1, \dots, \nDomains$ and $\indexUnit = 1, \dots, \nUnitIndexed$. The population of size $N$ is divided into $D$ non-overlapping small areas of sizes $N_i$ and into $n$ sampled and $N-n$ non-sampled units, denoted by $s$ and $r$ respectively. $y_{\indexDomain\indexUnit}$ is the the dependent variable for domain $\indexDomain$ and unit $\indexUnit$, and $\xUnit$ are the corresponding auxiliary information for that unit. Furthermore $\randomEffect$ and $\samplingError$ are independent. Let $\hat{\beta}$ denote the best linear unbiased estimator (BLUE) of $\beta$ and $\hat{\randomEffectIndexed}$ the best linear unbiased predictor (BLUP) of $\randomEffectIndexed$ (cf.\ \citealp{Hen50} or \citealp{Sea71}). The empirical best linear unbiased predictor (EBLUP) of the mean in small area $i$ in the Battese-Harter-Fuller model is then given by
\begin{eqnarray}\label{BHFEBLUP}\hat{\overline{y}}_i^{BHF}&=&N_i^{-1}\Big\{\sum_{j\in s_i}y_{ij}+\sum_{j\in r_i}(\xUnit^\top\hat{{\beta}}+\hat{\randomEffectIndexed})\Big\}.
\end{eqnarray}

Due to reasons of confidentiality unit-level information is not always available. Instead only aggregates, or rather the direct estimators may be supplied. However, these direct estimations are known to be unreliable, hence in such situations area-level models can be valuable. The area-level model introduced by \cite{fay79} is build on a sampling model:
%
\[\directStat_{\indexDomain} = \trueStat_{\indexDomain} + \samplingError_{\indexDomain},\]
%
where $\directStat_{\indexDomain}$ is a direct estimator of a statistic of interest $\trueStat_{\indexDomain}$ for an area $\indexDomain$. The sampling error $\samplingError_{\indexDomain}$ is assumed to be independent and normally distributed with known variances $\samplingVarianceIndexed$, i.e. $\samplingError_{\indexDomain}|\trueStat_{\indexDomain} \sim \mathit{N}(0, \samplingVarianceIndexed)$. The model is modified with a linking model by assuming a linear relationship between the true area statistic $\trueStat_{\indexDomain}$ and some auxiliary variables $\xArea$:
%
\[\trueStat_{\indexDomain} = \xArea^\top \beta + \randomEffectIndexed,\] 
%
with $\indexDomain=1,\dots, \nDomains$. 
%Note that $\xArea$ is a vector containing area-level (aggregated) information for $\nRegressor$ variables and $\beta$ is a vector ($1\times \nRegressor$) of regression coefficients describing the (linear) relationship. 
The model errors $\randomEffectIndexed$ are assumed to be independent and normally distributed, i.e. $\randomEffectIndexed \sim \mathit{N}(0, \randomEffectVariance)$. Furthermore $\samplingErrorIndexed$ and $\randomEffectIndexed$ are assumed to be independent. Combining the sampling and linking model leads to:
\begin{equation}
\label{eq:FH}
\directStatIndexed = \xArea^\top \beta + \randomEffectIndexed + \samplingErrorIndexed.
\end{equation} 
%
Model \ref{eq:FH} is effectively a random-intercept model where the distribution of the error term $\samplingErrorIndexed$ is heterogeneous and known. The EBLUP of the small area mean in the Fay-Herriot (FH) model is given by 
\begin{eqnarray}\label{FHEBLUP}\hat{\overline{y}}_i^{FH}&=&\xArea^\top \hat{\beta} + \hat{\randomEffectIndexed}.
\end{eqnarray}

\section{A simulation framework}
\label{sec:framework}
%
%% flow-diagram
\begin{wrapfigure}{R}{0.5\textwidth}
\begin{center}
\includegraphics[width=0.5\textwidth]{flowdiagram}
\end{center}
\caption{\label{fig:flowdiagram}Process of simulation. Left column are the steps in a simulation. Right column are the corresponding function names to represent those steps in \proglang{R}.} 
\end{wrapfigure}
%
In this section we will present the simulation framework implemented in \proglang{saeSim}. The framework relies strongly on the idea to describe a simulation as a process of data manipulation. Independent of simulation studies, \cite{wickham14a} as well as \cite{wickham14b} strongly promote this idea by providing tools for cleaning and transforming data. In those frameworks every defined function takes a \proglang{data.frame} as input and returns it modified. This leads to a natural connection between all defined functions as the result of one function can be directly passed to the next as an argument. The symbioses of these packages with the pipe operator (\proglang{\%>\%}) from the package \proglang{magrittr} \citep{bache14} only emphasises the process of data manipulation. 

In \proglang{saeSim} we want to extend this approach to simulation studies where we have the field of small area estimation in mind. The main focus lies on the description of a simulation as a process of data manipulation. Each step in this process can be defined as a self contained component (function) and thus easily replaced, extended and most importantly reused. Before we go into any detail of the functionality of the package we will discuss this process behind simulation studies followed by how \proglang{saeSim} maps this process into \proglang{R}.

Simulation studies in small area estimation address three different levels, the population, the sample and data on aggregated level. Figure \ref{fig:flowdiagram} illustrates these levels. In the left column the steps of data manipulation are described, the right column presents the function names to define those steps. The \textbf{population-level} defines the data on which a study is conducted and may be a true population, synthetic population data or randomly generated variates from a model. We see three different point of views to define a population. First \textit{design-based}, which means that a simulation study is based on true or synthetic data of \textit{one} population. Second a \textit{semi-model-based} point of view, where only one population is drawn from a model and is fixed in the whole simulation study. And third, \textit{model-based} studies which have changing random populations drawn from a model.

The scope of this article is not not to promote any of those viewpoints, but simply to identify the similarity in them. The \textit{base} (first component in figure \ref{fig:flowdiagram}) of any simulation study is a data table, the question is, if this data is \textit{fixed} or \textit{random} over the course of the simulation. Or from a more technical point of view, is the data generation (the second step in figure \ref{fig:flowdiagram}) repeated in each simulation run or omitted. Depending on the choice of a fixed or random population it is necessary to recompute the population domain-statistics like domain means and variances, or other statistics of interest (third component in figure \ref{fig:flowdiagram}).

The \textbf{sample-level} is when domain predictions are conducted for unit-level models. Independently of how the population is treated, fixed or random, this phase consists of two steps, first drawing a sample, and second conducting computations on the samples (fourth and fifth component in figure \ref{fig:flowdiagram}). Given a sample, design or model based small area methods are applied. Of interest are estimated parameters, which can be estimated model parameters or domain predictions as well as measures of uncertainty for the estimates.

As the sample-level is when unit-level models are applied, the \textbf{aggregate-level} is when area-level models are applied (the seventh and last component in figure \ref{fig:flowdiagram}). Area-level models in small area estimation typically only use information available for domains (in contrast to units). Thus, the question for simulation studies for area-level methods is, if the data is generated on unit-level and used after the aggregation (sixth component in figure \ref{fig:flowdiagram}) or if the data is generated directly on area-level, i.e. drawn from an area-level model. Depending on whether unit-level data and sampling are part of the simulation the aggregate-level follows the generation of the population or is based on the aggregated sample. Again, we do not promote a specific viewpoint but simply allow steps in the process of simulation to be omitted.

Depending on the topic of research, steps in this simulation framework can be more relevant than others or completely irrelevant. We see these steps more as a complete list of phases one can encounter, thus single components can be omitted if not relevant in specific applications. For example \textit{data generation} is not relevant if you have population data, or the \textit{sample-level} is not used, when the sample is directly drawn from the model.

From this considerations, \proglang{saeSim} maps the different steps into \proglang{R}. Two layers with separate responsibilities need to be discussed. The first is \textit{how} different simulation components can be combined, and the second is \textit{when} or in which order they are applied. Regarding the first, in \proglang{saeSim} we put a special emphasis on the interface of each component, which is to use functions which take a \proglang{data.frame} as argument and have a \proglang{data.frame} as return value. This definition of interfaces, the return value of one component is the input of the next, is used for all existing tools in \proglang{saeSim}.

The second column in figure \ref{fig:flowdiagram} shows how the different steps in a simulation can be accessed. It is important to note that the functions in figure \ref{fig:flowdiagram} control the process, the second layer, i.e. \textit{when} components are applied. Each of these functions take a simulation setup object to be modified and a function with the discussed interface as arguments. Hence a simulation setup is a collection of functions to be applied in a certain sequence. Also the second-layer functions have a defined interface: a \proglang{sim\_setup} as input to be modified and a \proglang{sim\_setup} as output. Thus, components can be chained together using the \textit{pipe operator} (\proglang{\%>\%}) from the package \proglang{magrittr}.

With \proglang{saeSim} we want to contribute tools for simulation studies in the field of small area estimation. We see the need for sharing tools for data generation and simulation amongst the scientific community and thus defined an interface for these tools as well as a platform to make them accessible. By defining the steps behind a simulation we hope to promote a reasonable way to communicate them alongside publications and during research. In the next section we present two case studies and introduce some of the main features provided by the package.

\section{Case studies}
\label{sec:caseStudy}
We will present two case studies, one model-based simulation in section \ref{sec:csModel} and a design-based simulation in section \ref{sec:csDesign}. Before, we want to introduce some basic functionalities as the pipe operator (\proglang{\%>\%}) needs some explanation, if unfamiliar. The pipe operator is designed to make otherwise nested expressions more readable as a line can be read from left to right, instead from inside out \citep{bache14}. As a minimal example see the following lines which are equivalent:

<<eval=FALSE>>=
sum(1:10)
1:10 %>% sum
@

In \proglang{saeSim} we rely on this operator, although all functions can be used without, we strongly recommend to use it. The following example shows some of the aspects of the package:

<<eval=FALSE>>=
setup1 <- sim_base_lm() %>% sim_sample(sample_number(5))
setup2 <- sim_base_lm() %>% sim_sample(sample_fraction(0.05))
@

Without knowing anything about the setup defined in \proglang{sim\_base\_lm} we can see that \proglang{setup1} and \proglang{setup2} only differ in the applied sampling scheme. \proglang{sim\_sample} is responsible to control when a function is applied (after the population-level) and \proglang{sample\_number(5)} and \proglang{sample\_fraction(0.05)} define the explicit way of drawing samples. Braking the responsibility of each component into what is applied and when it is applied makes it possible to add new components to any step in the process. The composition of a simulation in that manner will focus on the definition of components and hide control structures. Any function can be passed to \proglang{sim\_sample} which has a \proglang{data.frame} as input as well as return value. The only responsibility of that function is to draw a sample, which makes it easy to find, understand and reuse when published. The operator \proglang{\%>\%} is used to add new components to the setup. 

\subsection{Model-based simulation}
\label{sec:csModel}
<<echo=FALSE>>=
set.seed(1) # for reproduction of the results
@
In the following we want to show how to construct a simulation in a model-based setting. The goal is to estimate the domain predictions under a FH model. Components which are involved are \textit{data generation} and \textit{computing on aggregated data} (see figure \ref{fig:flowdiagram}). The first step is to generate the data under the model:

\[ y_i = 100 + 2 \cdot x_i + v_i + e_i\]

where $x_i \stackrel{iid}{\sim} N(0, 4^2)$, $v_i \stackrel{iid}{\sim} N(0, 1)$ and $e_i \stackrel{indep}{\sim} N(0, \sigma_i^2)$ with $\sigma_i^2 = 0.1, 0.2, \dots, 4$ and $i = 1, \dots, 40$ as index for the domains. Also are $x_i$, $v_i$ and $e_i$ independent from each other. The area-level data for the simulation is generated in each Monte Carlo repetition. 

In this case the \textit{base-component} is a data table with an id variable named \proglang{idD} and constructed with the function \proglang{base\_id}. Any random number generator in \proglang{R} can be used, however we have normally distributed variates, for which some predefined functions are available.

<<>>=
library(saeSim)
setup <- base_id(nDomains = 40, nUnits = 1) %>% 
  sim_gen_x(mean = 0, sd = 4) %>%
  sim_gen_v(mean = 0, sd = 1)
setup
@

Note that if you print a simulation setup to the console, as in the above example, one simulation run is performed and only the first rows (the head) of the resulting data table are printed. This enables interactivity with the object itself, however it hides that the setup object is a collection of functions to be called. In this model the error component $e_i$ has different variances which is not covered by a predefined function. Thus, as a \textit{generator component} we define a function which takes a \proglang{data.frame} as input and returns it after adding a variable named \proglang{vardir} with the variances and the variable \proglang{e} with the generated random numbers:

<<>>=
gen_e <- function(dat) {
  dat$vardir <- seq(0.1, 4, length.out = nrow(dat))
  dat$e <- rnorm(nrow(dat), sd = sqrt(dat$vardir))
  dat
}
setup <- setup %>% sim_gen(gen_e)
setup
@

The last step in data generation is to construct the response variable which will be named \proglang{y} and added to the data. Also we will add the \textit{true} area statistic under the model to the data:

<<>>=
setup <- setup %>% 
  sim_resp_eq(y = 100 + 2 * x + v + e) %>%
  sim_comp_pop(comp_var(trueStat = y - e))
@

To add the area-level predictions from a Fay-Herriot model we need to define another component. The function takes a \proglang{data.frame} as input and returns the modified version. For the estimation of the EBLUP under the FH model we use the function \proglang{eblupFH} from the package \proglang{sae} \citep{molina13}. Hence we define a function named \proglang{comp\_FH} and add it to the process:

<<>>=
library(sae)
comp_FH <- function(dat) {
  modelFH <- eblupFH(y ~ x, vardir, data = dat)
  dat$FH <- modelFH$eblup
  dat
}
setup <- setup %>% sim_comp_agg(comp_FH)
setup
@

The object \proglang{setup} stores all necessary information to run one iteration of the simulation. In the following $R = 100$ repetitions are performed, the result is a \proglang{list} of \proglang{data.frame}s. The function \proglang{rbind\_all} from the package \proglang{dplyr} is used to combine the resulting \proglang{list}:

<<>>=
library(dplyr)
simResults <- sim(setup, R = 100) %>% rbind_all
simResults %>% select(idD, idR, simName, trueStat, y, FH)
@

An additional variable \proglang{idR} is automatically added as an ID-variable for the iteration as well as a variable \proglang{simName} to distinguish between scenarios, should there be more than one. At this time we do not provide further tools to process the resulting data. As it is a \proglang{data.frame} many packages are available in \proglang{R}. In the design-based scenario we show how to process the result data into graphs with only a few lines of code.

\subsection{Design-based simulation}
\label{sec:csDesign}

In the design-based simulation we want to illustrate the use of \proglang{saeSim} when starting from data of a population. For this purpose we use a synthetic population generated from Austrian EU-SILC (European Union Statistics on Income and Living Conditions) data. The data consists of 25 thousand households and does not represent the true population size of Austria. It is published alongside the \proglang{R}-package \proglang{simFrame} \citep{Alf10} where it is used as an example data set. To keep this study as simple as possible, we further restrict the data for the main income holder and will only use some of the provided auxiliary information.

<<>>=
data(eusilcP, package = "simFrame")

simDat <- eusilcP %>% 
  mutate(agesq = age^2, eqIncome = as.numeric(eqIncome)) %>%
  filter(main) %>%
  select(region, eqIncome, age, agesq, gender)

head(simDat)
@

Using this data as population, we want to repeatedly draw samples from it, then predict the domain means by using a direct estimator and a unit-level model. The sampling design is to draw 10 per cent from each region with simple random sampling. For each region the direct estimator for income as well as the EBLUP under the BHF model is computed. Although the data offers some more information, we use only \proglang{gender}, \proglang{age} and \proglang{agesq} as predictor variables. The function \proglang{eblupBHF} from the package \proglang{sae} is an implementation of the BHF estimator and used as an example. This function expects three data objects and returns domain predictions. The sampled data, the population means of the auxiliary variables and the population sizes in each domain. Before we can begin to construct the simulation setup we store these data tables as attributes to the population data. There are other options but setting attributes to the processed data is a very flexible form of processing data on different aggregation levels.

<<>>=
attr(simDat, "popMeans") <- group_by(simDat, region) %>% 
  summarise(age = mean(age),
            agesq = mean(agesq),
            genderFemale = mean(as.integer(gender) - 1),
            trueStat = mean(eqIncome))

attr(simDat, "popMeans")

attr(simDat, "popN") <- group_by(simDat, region) %>% summarise(N = n())

attr(simDat, "popN")
@

Before we come to the estimation, the first step is to add a sampling scheme. As stated earlier, the starting point of a simulation setup is to provide a \proglang{data.frame} as \textit{base-component} which, in this case, is the population data. Then the sampling component is added, in which we define to draw 10 per cent of the observations from each domain with simple random sampling.

<<>>=
setup <- simDat %>% 
  sim_sample(sample_fraction(0.1, groupVars = "region"))
setup
@

What now needs to be done, is to define the components which add the desired estimates to the data. Here we will compute the mean of income in each domain as the direct estimator and the EBLUP under the BHF model. Although this could be done in one step, we will separate the two computations to illustrate how to combine several estimations and define each component independent of one another. This focus on the definition of each component and meeting the convention of the defined interface is the intended approach. It will automatically organise the simulation and each component is arranged using the simulation framework. Hence, we will define two functions, one for adding the direct estimates and one for adding the EBLUP.

<<>>=
comp_direct <- function(dat) {
  attr(dat, "sampleMean") <- 
    dat %>% group_by(region) %>% summarise(direct = mean(eqIncome))
  dat
}

comp_BHF <- function(dat) {
  popMeans <- select(attr(dat, "popMeans"), -trueStat)
  modelBHF <- 
    eblupBHF(eqIncome ~ age + agesq + gender, region, meanxpop = popMeans,
             popnsize = attr(dat, "popN"), data = dat)
  attr(dat, "BHF") <- modelBHF$eblup
  dat
}
@

Another positive aspect of the above definitions is, that each step is relatively small and the purpose clearly defined, which will make them easy to understand and reuse. Finally the simulation results are combined in an \textit{aggregation-component}, possibly followed by the application of area-level models but omitted in this example. The result of this aggregation step will be a \proglang{data.frame} with one row for each region.

<<>>=
agg_results <- function(dat) {
  cbind(attr(dat, "sampleMean"),
        BHF = attr(dat, "BHF")$eblup,
        trueStat = attr(dat, "popMeans")$trueStat)
}
@

To combine the simulation setup and the defined components we arrange them using the function \proglang{sim\_comp\_sample} to ensure that the direct estimator and the EBLUP are computed on the sampled data, and \proglang{sim\_agg} to add the above aggregation step.

<<>>=
setup <- setup %>%
  sim_comp_sample(comp_BHF) %>%
  sim_comp_sample(comp_direct) %>%
  sim_agg(agg_results)
setup
@

To repeat the simulation $R = 50$ times the simulation setup can be passed to the function \proglang{sim}, the resulting \proglang{list} is directly combined using \proglang{rbind\_all}.

<<>>=
simResults <- setup %>% sim(R = 50) %>% rbind_all
simResults
@

To further process the simulation results we present two plots using the package \proglang{ggplot2} \citep{wickham09} and \proglang{reshape2} \citep{wickham07} for further reshaping of the data. As can be seen, little effort is needed to process the result data. Figure \ref{fig:dbBIAS} and \ref{fig:dbMSE} show the Monte-Carlo BIAS and MSE for each region in Austria and for each estimator. Keep in mind that the BHF was applied to illustrate the simulation framework. There are a number of issues with regard to model choice and variable selection which we will not address any further.

<<>>=
library(reshape2)
ggDat <- melt(simResults,
              id.vars = c("region", "trueStat"), 
              measure.vars = c("BHF", "direct"), 
              variable.name = "method",
              value.name = "prediction")
@

<<eval=FALSE>>=
library(ggplot2)
ggplot(ggDat, aes(x = region, y = prediction - trueStat, fill = method)) + 
  geom_boxplot() + theme(legend.position = "bottom")
ggplot(ggDat, aes(x = region, y = (prediction - trueStat)^2, fill = method)) + 
  geom_boxplot() + scale_y_log10() + theme(legend.position = "bottom")
@

<<designSimBIAS, fig = TRUE, width = 8.3, height=4, include = FALSE, echo = FALSE>>=
library(ggplot2)
ggplot(ggDat, aes(x = region, y = prediction - trueStat, fill = method)) + 
  geom_boxplot() + theme(legend.position = "bottom")
@

\begin{figure}[!h]
\includegraphics[width = \textwidth]{saeSim-designSimBIAS.pdf}
\caption{Monte-Carlo BIAS of direct vs. BHF predictor. 50 predictions for each region and estimation technique.}
\label{fig:dbBIAS}
\end{figure}


<<designSimMSE, fig=TRUE, width = 8.3, height=4, include = FALSE, echo = FALSE>>=
ggplot(ggDat, aes(x = region, y = (prediction - trueStat)^2, fill = method)) + 
  geom_boxplot() + scale_y_log10() + theme(legend.position = "bottom")
@

\begin{figure}[!h]
\includegraphics[width = \textwidth]{saeSim-designSimMSE.pdf}
\caption{Monte Carlo MSE of direct vs. BHF predictor. 50 predictions for each region and estimation technique.}
\label{fig:dbMSE}
\end{figure}


  
\section{Outlook}
\label{sec:outlook}
With \proglang{saeSim} we have three objectives, making simulation tools available and reusable, unify the process behind simulations and third assist the researcher to design her code in a transparent way. By providing the package we not only want to make our own tools available but also open it for contributions. The package source is available on CRAN (\url{http://cran.r-project.org/web/packages/saeSim/}) and the repository for development on GitHub (\url{https://github.com/wahani/saeSim}). As GitHub allows to share and contribute source code using version-control, it is open for submissions. Apart from the availability of specific utility functions, we hope to promote and support the design of source code for simulation studies. One aspect is the design of simulations as processes of data. Furthermore, we encourage the definition of small and self contained components, i.e. functions, as this reduces the lines of code necessary to be read in order to understand it's purpose. 

The package already provides more than introduced in this article. To mention is the support of outlier contaminated data. At this time only representative outliers (outlying observations in the population) are supported \citep[cf.][]{Cha86}. However, we plan to extend this feature to non-representative outliers (outliers are part of the sample but not the population). Furthermore, the inclusion of available random number generators available in the R-language is already possible and access to generate group effects as needed in mixed models is supported. Also, as the response is created by an \proglang{R} expression, any form of non-linearity in the relationship between response and auxiliary variables as well as error components can be modelled.

A more technical feature is a back-end for parallel computations which is a link to the \proglang{parallel} package in \proglang{R} \citep{r14}. Tools to process result data after the simulation as well as plotting methods for that data are an avenue for further research. Already available are some simple plots for the simulation setups as well as a summary method to get information on the expected runtime and structure of the resulting data.


%\bibliographystyle{plainat}
\bibliography{saeSim}



\end{document}
